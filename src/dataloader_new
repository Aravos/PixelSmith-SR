import sys
import torch
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from torchvision.utils import make_grid
from tqdm import tqdm
import os
from wgan_gp import Generator, Critic, initialize_weights
from dataloader_DIV2K import PatchIterableDataset
from utils import gradient_penalty

LEARNING_RATE = 1e-4
PATCH_SIZE = 512
BATCH_SIZE = 1
CHANNELS_IMG = 3
ADDITIONAL_EPOCHS = 5
GEN_FEATURES = 256
CRITIC_FEATURES = 64
CRITIC_ITERATIONS = 5
LAMBDA_GP = 10

device = "cuda" if torch.cuda.is_available() else "cpu"

HR_TRAIN_DIR = "/home/aravos/Code/02-Upscale-Project/Image-Upscaler/dataset/DIV2K/DIV2K_train_HR"
LR_TRAIN_DIR = "/home/aravos/Code/02-Upscale-Project/Image-Upscaler/dataset/DIV2K/DIV2K_train_LR_blurred"
VALID_HR_DIR = "/home/aravos/Code/02-Upscale-Project/Image-Upscaler/dataset/DIV2K/DIV2K_valid_HR"
VALID_LR_DIR = "/home/aravos/Code/02-Upscale-Project/Image-Upscaler/dataset/DIV2K/DIV2K_valid_LR_blurred"

save_dir = "/home/aravos/Code/02-Upscale-Project/Image-Upscaler/checkpoints"
os.makedirs(save_dir, exist_ok=True)
writer_real = SummaryWriter("/home/aravos/Code/02-Upscale-Project/Image-Upscaler/logs-WGAN/real")
writer_fake = SummaryWriter("/home/aravos/Code/02-Upscale-Project/Image-Upscaler/logs-WGAN/fake")
writer_val  = SummaryWriter("/home/aravos/Code/02-Upscale-Project/Image-Upscaler/logs-WGAN/val")

gen = Generator(channels_img=CHANNELS_IMG, features_g=GEN_FEATURES).to(device)
critic = Critic(channels_img=CHANNELS_IMG, features_d=CRITIC_FEATURES).to(device)
initialize_weights(gen)
initialize_weights(critic)

opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.9))
opt_critic = optim.Adam(critic.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.9))

start_epoch = 0
checkpoint_files = [f for f in os.listdir(save_dir) if f.startswith("gan_epoch_") and f.endswith(".pth")]
if checkpoint_files:
    latest_checkpoint = max(checkpoint_files, key=lambda x: int(x.split("_")[-1].split(".")[0]))
    checkpoint_path = os.path.join(save_dir, latest_checkpoint)
    print(f"Loading checkpoint: {checkpoint_path}")
    checkpoint = torch.load(checkpoint_path, map_location=device)
    gen.load_state_dict(checkpoint["gen_state_dict"])
    critic.load_state_dict(checkpoint["critic_state_dict"])
    opt_gen.load_state_dict(checkpoint["opt_gen_state_dict"])
    opt_critic.load_state_dict(checkpoint["opt_critic_state_dict"])
    start_epoch = checkpoint["epoch"] + 1
else:
    print("No checkpoint found, starting from scratch.")

gen.train()
critic.train()

total_epochs = start_epoch + ADDITIONAL_EPOCHS

val_dataset = PatchIterableDataset(hr_dir=VALID_HR_DIR, lr_dir=VALID_LR_DIR, patch_size=PATCH_SIZE)
val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=0)

step = 0

for epoch in range(start_epoch, total_epochs):
    train_dataset = PatchIterableDataset(hr_dir=HR_TRAIN_DIR, lr_dir=LR_TRAIN_DIR, patch_size=PATCH_SIZE)
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)
    
    loop = tqdm(enumerate(train_loader), total=len(train_loader), leave=True)
    for batch_idx, (lr_batch, hr_batch) in loop:
        lr_batch = lr_batch.to(device)
        hr_batch = hr_batch.to(device)
        
        B, P, C, H_lr, W_lr = lr_batch.shape
        lr_batch = lr_batch.view(B * P, C, 128, 128)
        hr_batch = hr_batch.view(B * P, C, 512, 512)

        for _ in range(CRITIC_ITERATIONS):
            fake_hr_flat = gen(lr_batch)
            critic_real = critic(hr_batch).reshape(-1)
            critic_fake = critic(fake_hr_flat.detach()).reshape(-1)
            gp = gradient_penalty(critic, hr_batch, fake_hr_flat, device)
            loss_critic = LAMBDA_GP * gp - (torch.mean(critic_real) - torch.mean(critic_fake))
            critic.zero_grad()
            loss_critic.backward(retain_graph=True)
            opt_critic.step()

        fake_hr_flat = gen(lr_batch)
        critic_fake_for_gen = critic(fake_hr_flat).reshape(-1)
        loss_gen = -torch.mean(critic_fake_for_gen)
        gen.zero_grad()
        loss_gen.backward()
        opt_gen.step()

        if batch_idx % 50 == 0:
            with torch.no_grad():
                fake_for_log_flat = gen(lr_batch)
                fake_for_log = fake_for_log_flat.view(B, P, C, 512, 512)
                img_grid_real = make_grid(hr_batch[:12], nrow=4, normalize=True)
                img_grid_fake = make_grid(fake_for_log_flat[:12], nrow=4, normalize=True)
                writer_real.add_image("Train/Real", img_grid_real, global_step=epoch * len(train_loader) + batch_idx)
                writer_fake.add_image("Train/Fake", img_grid_fake, global_step=epoch * len(train_loader) + batch_idx)
            
        loop.set_description(f"Epoch [{epoch+1}/{total_epochs}]")
        loop.set_postfix(Loss_D=loss_critic.item(), Loss_G=loss_gen.item())
        step += 1
        del lr_batch
        del hr_batch
        torch.cuda.empty_cache()

    gen.eval()
    with torch.no_grad():
        for val_batch_idx, (lr_val, hr_val) in enumerate(val_loader):
            lr_val = lr_val.to(device)
            hr_val = hr_val.to(device)
            B_val, P_val, C, H_lr, W_lr = lr_val.shape
            lr_val_flat = lr_val.view(B_val * P_val, C, H_lr, W_lr)
            fake_val_flat = gen(lr_val_flat)
            fake_val = fake_val_flat.view(B_val, P_val, C, 512, 512)
            real_grid_val = make_grid(hr_val.view(B_val * P_val, C, 512, 512), nrow=4, normalize=True)
            fake_grid_val = make_grid(fake_val_flat, nrow=4, normalize=True)
            writer_val.add_image("Validation/Real", real_grid_val, global_step=epoch)
            writer_val.add_image("Validation/Fake", fake_grid_val, global_step=epoch)
            break
    gen.train()

    checkpoint_path = os.path.join(save_dir, f"gan_epoch_{epoch+1}.pth")
    torch.save({
        'epoch': epoch+1,
        'gen_state_dict': gen.state_dict(),
        'critic_state_dict': critic.state_dict(),
        'opt_gen_state_dict': opt_gen.state_dict(),
        'opt_critic_state_dict': opt_critic.state_dict(),
    }, checkpoint_path)
    print(f"Model saved at epoch {epoch+1}")

print("Training Complete!")
